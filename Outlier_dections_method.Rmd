---
title: 'Homework 2: Outlier Detection Review'
author: "Patricio S. La Rosa"
date: "2/23/2020"
output:
  pdf_document: default
  html_document:
    keep_md: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Deep-Dive on Outlier detection Methods

The purpose of this homework is to review several outlier detection methods made avialable through R packages. After completition of this homework you should be familiarized with standard methods for univariate and multivariate outlier detection methods. Application of these techniques to you project, as it applies, will be requested and will be considered as part of your mid-term report.  

For more details on how to work with RmarkDown please read the following link:
https://www.stat.cmu.edu/~cshalizi/rmarkdown/

Please install the following packages prior to execute the R Markdown:
install.packages(c("OutlierDetection","OutliersO3","outliers"))

```{r}
library(OutlierDetection)
library(OutliersO3)
library(outliers)
```


## Data Description:

We will proceed now to summarize the classical Toy Example iris:

The Fisher's or Anderson's iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. For more information about the data set, execute 

```{r iris}
help(iris)
summary(iris)
head(iris)

```


## Problem 1: Expanding knowledge based on Outlier detection techniques
The objective of this problem is to expose the student to different outlier detection techniques made available through R packages. The goal is to ensure that the main assumptions of these techniques are learned and the students is capable of articulating how the technique works statistically and in practice by using a toy example.

As discussed in our lecture, outlier detection techniques can be classified as follows:

1.-Statistical Tests based Approaches
2. Depth-based Approaches
3. Deviation-based Approaches
4. Distance-based Approaches
5. Density-based Approaches

Your task is to complete this Rmarkdown with a technical summary describing each of the technique entitled below and use the toy example to describe its application.


### 1.-Statistical Tests based Approaches:

#### a) Dixon test (small sample size)

Technical Summary:
The Dixon test can be used to test for a single outlier in a univariate data set. This test is primarily used for small data sets (between 3 and 30). It can be used to test whether the minimum value is an outlier, the maximum value is an outlier, or either the minimum or maximum value is an outlier. The Dixon test is based on comparing the distance of one end observation from its neighbors with the range of all the observations (or all but one or two observations). 

References:
- Dixon, W.J. (1950). Analysis of extreme values. Ann. Math. Stat. 21, 4, 488-506.
- Dixon, W.J. (1951). Ratios involving extreme values. Ann. Math. Stat. 22, 1, 68-78.
- Rorabacher, D.B. (1991). Statistical Treatment for Rejection of Deviant Values: Critical Values of Dixon Q Parameter and Related Subrange Ratios at the 95 percent Confidence Level. Anal. Chem.
83, 2, 139-146.

Application:
dixon.test(x, type = 0, opposite = FALSE, two.sided = TRUE)
x is a numeric vector for data values. opposite is a logical indicating whether you want to check not the value with largest difference from the mean, but opposite (lowest, if most suspicious is highest etc.). type is an integer specyfying the variant of test to be performed. Possible values are compliant with these given by Dixon (1950): 10, 11, 12, 20, 21. If this value is set to zero, a variant of the test is chosen according to sample size (10 for 3-7, 11 for 8-10, 21 for 11-13, 22 for 14 and more). The lowest or highest value is selected automatically, and can be reversed used opposite parameter. two.sided treats test as two-sided (default).
In this example, X is the first 30 rows of the first feature (Sepal.Length). 'type = 0' means the variant of the test is chosen according to sample size. 'opposite = TRUE' means to check the lowest value. The result show that Q = 0.21429, p-value = 0.7572. So the lowest value 4.3 is an outlier.

```{r}
X=iris[1:30,1]
dixon.test(X,type=0,opposite=TRUE)

```
#### b) Normalscore (Deviation with respect to the mean)

Technical Summary: The Z score associated with the largest value of X in a data set of size n, is shown to be bounded above by $(n-1)/\sqrt{n}$. As a result, outliers defined as values exceeding four standard deviations from the mean cannot exist for small data sets.


References:
Schiffler, R.E (1998). Maximum Z scores and outliers. Am. Stat. 42, 1, 79-80.

Application: scores(x, type = c("z", "t", "chisq", "iqr", "mad"), prob = NA, lim = NA) x is a vector of data. In "type", "z" calculates normal scores (differences between each value and the mean divided by sd), "t" calculates t-Student scores (transformed by $(z*\sqrt{n-2})/\sqrt{z-1-t^2}$ formula), "chisq" gives chi-squared scores (squares of differences between values and mean divided by variance. For the "iqr" type, all values lower than first and greater than third quartile is considered, and difference between them and nearest quartile divided by IQR are calculated. For the values between these quartiles, scores are always equal to zero. "mad" gives differences between each value and median, divided by median absolute deviation. For "prob", if set, the corresponding p-values instead of scores are given. If value is set to 1, p-value are returned. Otherwise, a logical vector is formed, indicating which values are exceeding specified probability. In "z" and "mad" types, there is also possibility to set this value to zero, and then scores are confirmed to $(n-1)/\sqrt{n}$ value, according to Shiffler (1998). The "iqr" type does not support probabilities, but "lim" value can be specified. "lim" can be set for "iqr" type of scores, to form logical vector, which values has this limit exceeded. In this example, 'type = "z"' means the normalised scores based on "z", and 'prob = 0.95' means that the percentile is 95%.

```{r}
X=iris[,1:4]
#scores(X,type="z",prob=0.95)
#Displaying first 10 scores
scores(X,type="z",prob=0.95)[1:10,]

```


#### c) Median Absolute Deviation (Deviation with respect to the median)

Technical Summary: The Z score associated with the largest value of X in a data set of size n, is shown to be bounded above by $(n-1)/\sqrt{n}$. As a result, outliers defined as values exceeding four median absolute deviation from the median cannot exist for small data sets.


References:
Schiffler, R.E (1998). Maximum Z scores and outliers. Am. Stat. 42, 1, 79-80.

```{r}
X=iris[,1:4]
#scores(X,type="mad",prob=0.95)
#Displaying first 10 scores
scores(X,type="mad",prob=0.95)[1:10,]

```


#### d) Interquantile range score

Technical Summary: The Z score associated with the largest value of X in a data set of size n, is shown to be bounded above by $(n-1)/\sqrt{n}$. For interquantile range score, all values lower than first and greater than third quartile is considered, and difference between them and nearest quartile divided by IQR are calculated.


References:
Schiffler, R.E (1998). Maximum Z scores and outliers. Am. Stat. 42, 1, 79-80.

Note: check for the value of limit to be used. Below I inserted an arbitrary value
```{r}
X=iris[,1:4]
#scores(X,type="iqr",lim=1)
#Displaying first 10 scores
scores(X,type="iqr",lim=1)[1:10,]
```


### 2. Depth-based Approach:

Technical Summary: Depth-based approach detects outlying data points in a 2-D dataset by, based on some definition of depth, organizing the data points in layers, with the expectation that shallow layers are more likely to contain outlying points than are the deep layers. FDC computes the first k 2-D depth contours by restricting the computation to a small selected subset of data points. It scales up much better than ISODEPTH. Also, FDC is robust against collinear points.


Reference:
Johnson, T., Kwok, I., and Ng, R.T. 1998. Fast computation of 2-dimensional depth contours. In Proc. Int. Conf. on Knowledge Discovery and Data Mining (KDD), New York, NY. Kno

Application: depthout(x, rnames = FALSE, cutoff = 0.05, boottimes = 100) x is the dataset for which outliers are to be found. rnames is the logical value indicating whether the dataset has rownames, default value is False. cutoff is the percentile threshold used for depth, default value is 0.05. boottimes is the number of bootsrap samples to find the cutoff, default is 100 samples. In this example, "cutoff = 0.05" means the percentile threshold used for depth is 0.05. From the results, we know the locations of outliers are 14, 23, 38, 42, 110, 118, 119, 132.

```{r}
X=iris[,1:4]
depthout(X,cutoff=0.05)



```

### 3. Deviation-based Approaches
Technical Summary: Deviation-based outlier detection does not use statistical tests or distance-based measures to identify exceptional objects. Instead, it identifies outliers by examining the main characteristics of objects in a group.

References:
A. Arning, R. Agrawal, and P. Raghavan. A linear method for deviation detection in large
databases. In Proc. 2nd International Conference on Knowledge Discovery and Data Mining,
1996
Chaudhary, A., Szalay, A. S., and Moore, A. W. 2002. Very fast outlier detection in large multidimensional data sets. In Proceedings of the ACM SIGMOD Workshop in Research Issues in Data Mining and Knowledge Discovery (DMKD). ACM Press


### 4. Distance-based Approaches
#### a) Outlier detection using Mahalanobis Distance
Technical Summary: Mahalanobis Distance (MD) is an effective distance metric that finds the distance between point and a distribution. It is quite effective on multivariate data. The reason why MD is effective on multivariate data is because it uses covariance between variables in order to find the distance of two points. In other words, Mahalanobis calculates the distance between point “P1” and point “P2” by considering standard deviation (how many standard deviations P1 far from P2). MD also gives reliable results when outliers are considered as multivariate. In order to find outliers by MD, distance between every point and center in n-dimension data are calculated and outliers found by considering these distances.

References:
Barnett, V. 1978. The study of outliers: purpose and model. Applied Statistics, 27(3), 242–250.

Application: maha(x, cutoff = 0.95, rnames = FALSE). X is a dataset for which outliers are to be found. cutoff is the percentile threshold used for distance, default value is 0.95. rnames is the logical value indicating whether the dataset has rownames, default value is False. maha computes Mahalanibis distance an observation and based on the Chi square cutoff, labels an observation as outlier. Outlierliness of the labelled 'Outlier' is also reported based on its p values. For bivariate data, it also shows the scatterplot of the data with labelled outliers.

```{r}
X=iris[,1:4]
maha(X,cutoff=0.9)
```

#### b) Outlier detection using k Nearest Neighbours Distance method
Technical Summary: The proposed kNN method detects outliers by exploiting the relationship among neighborhoods in data points. The farther a data point is beyond its neighbors, the more possible the data is an outlier. 

References:
Hautamaki, V., Karkkainen, I., and Franti, P. 2004. Outlier detection using k-nearest neighbour graph. In Proc. IEEE Int. Conf. on Pattern Recognition (ICPR), Cambridge, UK.

Application: nn(x, k = 0.05 * nrow(x), cutoff = 0.95, Method = "euclidean", rnames = FALSE, boottimes = 100). x is the dataset for which outliers are to be found. k is the number of nearest neighbours to be used, default value is 0.05*nrow(x). cutoff is the percentile threshold used for distance, default value is 0.95. method is the distance method, default is Euclidean. rnames is the logical value indicating whether the dataset has rownames, default value is False. boottimes is the number of bootsrap samples to find the cutoff, default is 100 samples. nn computes average knn distance of observation and based on the bootstrapped cutoff, labels an observation as outlier. Outlierliness of the labelled 'Outlier' is also reported and it is the bootstrap estimate of probability of the observation being an outlier. For bivariate data, it also shows the scatterplot of the data with labelled outliers.


```{r}
X=iris[,1:4]
nn(X,k=4)
```

#### c) Outlier detection using kth Nearest Neighbour Distance method
Technical Summary: Hautamaki propose two density-based outlier detection methods. In the first method, a vector is defined as an outlier if it participates in at most T neighbourhoods in kNN graph, where threshold T is a control parameter. To accomplish this we consider kNN graph as a directed proximity graph, where the vectors are vertices of the graph and edges are distances between the vectors. We classify a vector as outlier on basis of its indegree number in the graph. The second method, a modification of RRS, sorts all vectors by their average kNN distances, for which a global threshold T is defined. Vectors with large average kNN-distance are all marked as outliers.

References:
Hautamaki, V., Karkkainen, I., and Franti, P. 2004. Outlier detection using k-nearest neighbour graph. In Proc. IEEE Int. Conf. on Pattern Recognition (ICPR), Cambridge, UK.

Application: nnk(x, k = 0.05 * nrow(x), cutoff = 0.95, Method = "euclidean", rnames = FALSE, boottimes = 100). x is the dataset for which outliers are to be found. k is the number of nearest neighbours to be used, default value is 0.05*nrow(x). cutoff is the percentile threshold used for distance, default value is 0.95. method is the distance method, default is Euclidean. rnames is the logical value indicating whether the dataset has rownames, default value is False. boottimes is the number of bootsrap samples to find the cutoff, default is 100 samples. nnk computes kth nearest neighbour distance of an observation and based on the bootstrapped cutoff, labels an observation as outlier. Outlierliness of the labelled 'Outlier' is also reported and it is the bootstrap estimate of probability of the observation being an outlier. For bivariate data, it also shows the scatterplot of the data with labelled outliers.

```{r}
X=iris[,1:4]
nnk(X,k=4)
```


### 5. Density-based Approaches
#### a) Outlier detection using Robust Kernal-based Outlier Factor(RKOF) algorithm
Technical Summary: Ester presented the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it.

Reference:
Ester, M., Kriegel, H.-P., Sander, J., and Xu, X. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. Int. Conf. on Knowledge Discovery and Data Mining (KDD), Portland, OR.

Application: dens(x, k = 0.05 * nrow(x), C = 1, alpha = 1, sigma2 = 1, cutoff = 0.95, rnames = F, boottimes = 100). x is the dataset for which outliers are to be found. k is the number of nearest neighbours to be used, default value is 0.05*nrow(x). C	is the multiplication parameter for k-distance of neighboring observations. Act as bandwidth increaser. Default is 1 such that k-distance is used for the gaussian kernel. alpha is the sensivity parameter for k-distance/bandwidth. Small alpha creates small variance in RKOF and vice versa. Default is 1. sigma2 is the variance parameter for weighting of neighboring observations. cutoff is the percentile threshold used for distance, default value is 0.95. rnames is the logical value indicating whether the dataset has rownames, default value is False. boottimes is the number of bootsrap samples to find the cutoff, default is 100 samples. dens computes outlier score of an observation using DDoutlier package(based on RKOF algorithm) and based on the bootstrapped cutoff, labels an observation as outlier. Outlierliness of the labelled 'Outlier' is also reported and it is the bootstrap estimate of probability of the observation being an outlier. For bivariate data, it also shows the scatterplot of the data with labelled outliers.

```{r}
X=iris[,1:4]
dens(X,k=4,C=1)
```
#### b) Outlier detection using genralised dispersion
Technical Summary: Jin proposed a novel method to efficiently find the top-n local outliers in large databases. The concept of "micro-cluster" is introduced to compress the data. An efficient micro-cluster-based local outlier mining algorithm is designed based on this concept. As their algorithm can be adversely affected by the overlapping in the micro-clusters, we proposed a meaningful cut-plane solution for overlapping data. 

Reference:
Jin, W., Tung, A., and Han, J. 2001. Mining top-n local outliers in large databases. In Proc. ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (SIGKDD), San Francisco, CA.

Application: disp(x, cutoff = 0.95, rnames = FALSE, boottimes = 100). x	is the dataset for which outliers are to be found. cutoff is the percentile threshold used for distance, default value is 0.95. rnames is the logical value indicating whether the dataset has rownames, default value is False. boottimes is the number of bootsrap samples to find the cutoff, default is 100 samples. disp computes LOO dispersion matrix for each observation(dispersion matrix without cosidering the current observation) and based on the bootstrapped cutoff for score(difference between determinant of LOO dispersion matrix and det of actual dispersion matrix), labels an observation as outlier. Outlierliness of the labelled 'Outlier' is also reported and it is the bootstrap estimate of probability of the observation being an outlier. For bivariate data, it also shows the scatterplot of the data with labelled outliers.

```{r}
X=iris[,1:4]
disp(X,cutoff=0.99)
```

### 6. Join assessment of outlier detection methods using techniques described under 2 to 5.

Technical Summary: Given the abudance of method to define outliers a most recent strategy is to develop consensus outlier detection method. For example, rules such as majority vote can be applied when the techniques considered are essentially different. Per instance, see "Outlier detection" package function OutlierDetection which finds outlier observations for the data using different methods and labels an observation as outlier based on the intersection of all the methods considered. Using the function edit in R investigate the criterion being used and which techniques were considered. Also, proposed a modification to the function so to consider any technique to include any given number of techniques for outlier detection. Per instance, ensure that you can include the techniques covered under category 1.

Application: OutlierDetection(x, k = 0.05 * nrow(x), cutoff = 0.95, Method = "euclidean", rnames = FALSE, depth = FALSE, dense = FALSE, distance = FALSE, dispersion = FALSE). x is the dataset for which outliers are to be found. k is the number of nearest neighbours to be used for for outlier detection using bootstrapping, default value is 0.05*nrow(x). cutoff is the percentile threshold used for distance, default value is 0.95. Method is the distance method, default is Euclidean. rnames is the logical value indicating whether the dataset has rownames, default value is False. depth is the logical value indicating whether depth based method should be used or not, default is False. dense is the logical value indicating whether density based method should be used or not, default is False. distance is the logical value indicating whether distance based methods should be used or not, default is False. dispersion is the logical value indicating whether dispersion based methods should be used or not, default is False. It takes a dataset and finds its outliers using combination of different method.

```{r}
X=iris[,1:4]
OutlierDetection(X)
#Unveil the criterion used in OutlierDection function to define outliers using different methods
#edit(OutlierDetection) # uncomment and execute this line
```

## Problem 2: 
Apply the technique discussed above to the data set that you are using as part of the your problem. Please make sure to report the following:

a) summary of you data sets
Consider using summary function and use graphics to display your data

Our project is to predict whether a patient will eventually survive or die based on monitoring data from the first 48 hours of admission to the ICU. Because each patient's test feature is different, and the observation time of each feature is different, this data set is completely irregular, so unlike a normal data set, this data set has no regular observation data. In this practice, I only select one feature (NISysABP) to analysis the outlier.

First, we import data from csv file:
```{r}
library(readr)
NISysABP_copy <- read_csv("E:\\WashU\\ESE527 Practicum in Data Analytics & Statistics\\Practice 2\\NISysABP.csv")
NISysABP = as.matrix(NISysABP_copy)
```

Becasue the NISysABP of each patient's are completely independent, so there isn't a suitable summary function for this data feature. Hence, we use the histogram to display the data.

```{r}
hist(NISysABP,xlab = "NISysABP")
```

b) Apply all the outlier detection methods described above to your data set as they fit
Dixon test:
```{r}
dixon.test(NISysABP[1:30],type=0,opposite=TRUE)
```

Normalscore (Deviation with respect to the mean):
```{r}
scores(NISysABP,type="z",prob=0.95)[1:10,]
```

Median Absolute Deviation (Deviation with respect to the median):
```{r}
scores(NISysABP,type="mad",prob=0.95)[1:10,]
```

Interquantile range score:
```{r}
#Displaying first 10 scores
scores(NISysABP,type="iqr",lim=1)[1:10,]
```

Outlier detection using Mahalanobis Distance:
```{r}
maha(NISysABP,cutoff=0.9)
```

Outlier detection using k Nearest Neighbours Distance method:
```{r}
nn(NISysABP,k=4)
```

Outlier detection using kth Nearest Neighbour Distance method:
```{r}
nnk(NISysABP,k=4)
```

c) Report outlier based on consensus rule based on all the techniques that applied to your data sets.

```{r}
OutlierDetection(NISysABP)
#Unveil the criterion used in OutlierDection function to define outliers using different methods
#edit(OutlierDetection) # uncomment and execute this line
```









